# app.py
import cv2
import av
import mediapipe as mp
import numpy as np
import threading
import time
from pathlib import Path
from datetime import datetime
import streamlit as st
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, RTCConfiguration

# ---------------- ì„¤ì • ----------------
CAPTURE_DIR = Path("captures")
CAPTURE_DIR.mkdir(exist_ok=True)
RTC_CONFIGURATION = RTCConfiguration({"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]})

# ì „ì—­ ìº¡ì²˜ ì €ì¥ì†Œ (thread-safe)
LATEST_CAPTURE = {"bytes": None, "fname": None, "ts": None}
CAP_LOCK = threading.Lock()

# ---------------- Shaka íŒë³„ í•¨ìˆ˜ ----------------
def is_shaka(hand, w, h):
    """MediaPipe hand landmarks ê¸°ë°˜ ìƒ¤ì¹´ íŒë³„ (thumb and pinky up, others down)."""
    def c(i):
        lm = hand.landmark[i]
        return int(lm.x * w), int(lm.y * h)

    thumb_tip = c(4); thumb_ip = c(3)
    index_tip = c(8); index_kn = c(5)
    middle_tip = c(12); middle_kn = c(9)
    ring_tip = c(16); ring_kn = c(13)
    pinky_tip = c(20); pinky_kn = c(17)

    thumb_up = thumb_tip[1] < thumb_ip[1]         # ì—„ì§€ í´ì§
    pinky_up = pinky_tip[1] < pinky_kn[1]         # ìƒˆë¼ í´ì§

    index_down  = index_tip[1] > index_kn[1]
    middle_down = middle_tip[1] > middle_kn[1]
    ring_down   = ring_tip[1] > ring_kn[1]

    return thumb_up and pinky_up and index_down and middle_down and ring_down

# ---------------- VideoProcessor ----------------
class VideoProcessor(VideoProcessorBase):
    def __init__(self):
        # ê° worker/ìŠ¤ë ˆë“œ ë³„ë¡œ Mediapipe ê°ì²´ë¥¼ ìƒì„±
        self.mp_face = mp.solutions.face_detection
        self.mp_hands = mp.solutions.hands
        self.mp_draw = mp.solutions.drawing_utils

        self.face_detector = self.mp_face.FaceDetection(model_selection=0, min_detection_confidence=0.6)
        self.hand_detector = self.mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.6)
        self.captured = False
        self.last_capture_time = 0.0

    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        img = frame.to_ndarray(format="bgr24")
        h, w = img.shape[:2]
        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        # Mediapipe ì²˜ë¦¬
        face_res = self.face_detector.process(rgb)
        hand_res = self.hand_detector.process(rgb)

        face_detected = face_res.detections is not None
        shaka_detected = False

        # ì† ì²˜ë¦¬: ìƒ¤ì¹´ ì²´í¬
        if hand_res.multi_hand_landmarks:
            for hand_landmarks in hand_res.multi_hand_landmarks:
                # Draw landmarks
                self.mp_draw.draw_landmarks(img, hand_landmarks, self.mp_hands.HAND_CONNECTIONS)

                if is_shaka(hand_landmarks, w, h):
                    shaka_detected = True
                    # ì¤‘ì•™ í…ìŠ¤íŠ¸ (ê°„ë‹¨ í‘œì‹œ)
                    cv2.putText(img, "Shaka!", (w//2 - 140, h//2), cv2.FONT_HERSHEY_SIMPLEX, 2.0, (0,255,0), 6)
                    break

        # ì–¼êµ´ ë°•ìŠ¤ ê·¸ë¦¬ê¸°
        if face_detected:
            for d in face_res.detections:
                self.mp_draw.draw_detection(img, d)

        # ìº¡ì²˜ ì¡°ê±´: ì–¼êµ´ + ìƒ¤ì¹´ + (ë””ë°”ìš´ìŠ¤: 1ì´ˆ)
        if face_detected and shaka_detected and not self.captured:
            now = time.time()
            # ê°„ë‹¨ ë””ë°”ìš´ìŠ¤: ë§ˆì§€ë§‰ ìº¡ì²˜ë¡œë¶€í„° 1ì´ˆ ì´ìƒ ì§€ë‚˜ì•¼ í—ˆìš©
            if now - self.last_capture_time > 1.0:
                fname = CAPTURE_DIR / f"shaka_{int(now)}.jpg"
                # ì €ì¥(ì„œë²„ ë‚´)
                cv2.imwrite(str(fname), img)
                self.last_capture_time = now
                self.captured = True

                # ëª¨ë“ˆ ì „ì—­ ë³€ìˆ˜ì— JPEG bytes ì €ì¥ (thread-safe)
                _, jpg = cv2.imencode('.jpg', img)
                with CAP_LOCK:
                    LATEST_CAPTURE["bytes"] = jpg.tobytes()
                    LATEST_CAPTURE["fname"] = str(fname)
                    LATEST_CAPTURE["ts"] = datetime.fromtimestamp(now).isoformat(timespec='seconds')

        # ë¦¬ì…‹: ìƒ¤ì¹´ê°€ ì•ˆ ë³´ì´ë©´ ì¬ì´¬ì˜ ê°€ëŠ¥
        if not shaka_detected:
            self.captured = False

        return av.VideoFrame.from_ndarray(img, format="bgr24")

# ---------------- Streamlit UI ----------------
st.set_page_config(page_title="Shaka Shot", layout="centered")
st.title("ğŸ¤™ Shaka Shot â€” ìë™ ì´¬ì˜ ì•± (Streamlit + streamlit-webrtc)")

col1, col2 = st.columns([3,1])

with col1:
    st.markdown("**ì¹´ë©”ë¼ ìŠ¤íŠ¸ë¦¼** â€” ë¸Œë¼ìš°ì €ì—ì„œ ì¹´ë©”ë¼ ê¶Œí•œ í—ˆìš© í•„ìš”")
    ctx = webrtc_streamer(
        key="shaka-shot",
        rtc_configuration=RTC_CONFIGURATION,
        video_processor_factory=VideoProcessor,
        media_stream_constraints={"video": True, "audio": False},
        async_processing=True,
    )

with col2:
    st.markdown("**ì„¤ì • / ìº¡ì²˜**")
    st.write("- ì–¼êµ´ ê°ì§€ + Shaka(ì—„ì§€+ìƒˆë¼ í´ì§) ì¸ì‹ ì‹œ ìë™ ìº¡ì²˜")
    st.write("- ìº¡ì²˜ íŒŒì¼ì€ ì„œë²„ì˜ `captures/` í´ë”ì— ì €ì¥")
    st.write("- 'Refresh' ë²„íŠ¼ìœ¼ë¡œ ìµœì‹  ìº¡ì²˜ í™•ì¸")
    st.write("- 'Download' ìœ¼ë¡œ íŒŒì¼ ì €ì¥")

    if st.button("Refresh latest capture"):
        with CAP_LOCK:
            if LATEST_CAPTURE["bytes"] is not None:
                st.image(LATEST_CAPTURE["bytes"], caption=f"Latest: {LATEST_CAPTURE['fname']} ({LATEST_CAPTURE['ts']})")
                st.download_button("Download latest", data=LATEST_CAPTURE["bytes"], file_name=Path(LATEST_CAPTURE["fname"]).name, mime="image/jpeg")
            else:
                st.info("ì•„ì§ ìº¡ì²˜ëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")

    st.write("---")
    st.write("ê°œë°œì ë©”ëª¨:")
    st.write(" - í¬ì¦ˆ ê°ì§€ ë¯¼ê°ë„ëŠ” ì¡°ëª…/ì¹´ë©”ë¼ ê°ë„ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ.")
    st.write(" - í•„ìš”í•œ í™•ì¥: ì¢Œ/ìš° ì† êµ¬ë¶„, ì¹´ìš´íŠ¸ë‹¤ìš´, ì˜¤ë””ì˜¤ ì•Œë¦¼ ë“±")

# ìë™ìœ¼ë¡œ ìƒˆ ìº¡ì²˜ê°€ ë“¤ì–´ì™”ëŠ”ì§€ UIì—ì„œ ì£¼ê¸°ì ìœ¼ë¡œ í™•ì¸í•˜ê³  ì‹¶ë‹¤ë©´,
# streamlit.experimental_set_query_params / st_autorefresh ë“±ì„ í™œìš©í•´ì„œ ìë™ ìƒˆë¡œê³ ì¹¨ ì¶”ê°€ ê°€ëŠ¥.
